# 学习笔记--3.2 决策树
# SSS[课程链接](https://www.bilibili.com/video/BV17u411f7dt/?spm_id_from=333.788&vd_source=f20f12f3aeb414c35504dd8ea1cba544)

## 决策树
case：分类问题，回归问题
* 分类问题：如之前提到的如何训练标注，详细见PPT
* 回归问题：如之前提到的预测房价，详细见PPT

优势：
* 可解释的模型
* 可以处理数值和类别类特征

缺陷
* 非常不稳定，如：某一个节点出现问题会很大程度影响整体模型的效果
  * 解决方案：如集成学习，尽量穷举所有情况
* 复杂的决策树会导致过拟合（需要进行剪枝）
* 不能很好利用计算资源，结构决定了并行困难，当前决策依赖上一个决策

## 随机森林
```
目标：随机森林是为了解决决策树不稳定问题
原理：训练多个决策树来提升稳定性，每棵树独立训练，一起用于决策结果，比如投票or取平均值
缺陷：成本提升
随机性来自哪里：
* Bagging：有放回地随机抽样训练样本。如：随机抽取样本集合里面的样本数据标记后放回去
* 随机选择特征子集
```

## 梯度提升决策树
```
目标：梯度提升决策树是为了解决决策树不稳定问题
原理：
1. 串行顺序的训练多个树
2. 步骤
   a、初始模型训练，产出预测值 y
   b、在每一轮迭代中，计算上一个模型的残差，并用新的决策树拟合这些残差（目标值）（可以理解为模型调优的过程【优化算法的概念】），然后更新模型（学习器+1）。
```

## 总结
* 决策树：用于分类/回归问题的可解释模型
* 集成树以减少偏差和方差
  * 随机森林：并行训练多个具备随机性的树
  * 提督上升决策树：在残差上进行顺序训练
* 树模型在工业界广泛应用
  * 简单、易调优，通常能给出令人满意的结果


## 课后扩展
### 1.剪枝
复杂的决策树容易导致过拟合，因此需要进行剪枝。下面是一些关于决策树剪枝的关键点和方法：

#### 什么是过拟合？
过拟合是指模型在训练数据上表现得很好，但在测试数据或新数据上表现不佳。这通常是因为模型过于复杂，捕捉到了训练数据中的噪音和细节，而这些噪音和细节在新数据中不具有代表性。

#### 剪枝的目的
剪枝的目的是减少决策树的复杂度，从而提高模型在新数据上的泛化能力。通过剪枝，可以去掉一些不必要的分支，减少模型的复杂度，降低过拟合的风险。

#### 剪枝方法
剪枝主要有两种方法：预剪枝（Pre-pruning）和后剪枝（Post-pruning）。

##### 1. 预剪枝（Pre-pruning）
预剪枝是在构建决策树的过程中，通过设置一些停止条件来限制树的生长。例如：
- **最大深度（max depth）**：限制树的最大深度。
- **最小样本数（min samples split）**：在一个节点上进行分裂所需的最小样本数。
- **最小叶节点样本数（min samples leaf）**：一个叶节点所需的最小样本数。
- **最大特征数（max features）**：在每个分裂中考虑的最大特征数。

这些参数可以在构建决策树时设置，以防止树变得过于复杂。

##### 2. 后剪枝（Post-pruning）
后剪枝是在决策树完全构建之后，通过剪掉一些分支来简化树。常见的后剪枝方法包括：
- **代价复杂度剪枝（Cost Complexity Pruning）**：在剪枝过程中，通过引入一个代价复杂度参数（α）来平衡树的复杂度和误差。可以选择一个最佳的α值，使得剪枝后的树在交叉验证中表现最好。
- **错误率剪枝（Error Pruning）**：计算每个节点的错误率，如果剪掉某个节点能降低整体错误率，就剪掉该节点。

#### 实现示例
以下是使用Python中的`scikit-learn`库进行决策树剪枝的示例：

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# 加载数据集
data = load_iris()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建决策树分类器，设置预剪枝参数
clf = DecisionTreeClassifier(max_depth=3, min_samples_split=4, min_samples_leaf=2)

# 训练模型
clf.fit(X_train, y_train)

# 评估模型
train_score = clf.score(X_train, y_train)
test_score = clf.score(X_test, y_test)

print(f"训练集准确率: {train_score}")
print(f"测试集准确率: {test_score}")
```

通过设置`max_depth`、`min_samples_split`和`min_samples_leaf`等参数，可以有效地进行预剪枝，减少过拟合问题。

#### 总结
剪枝是防止决策树过拟合的重要技术。通过预剪枝和后剪枝，可以控制决策树的复杂度，提高模型的泛化能力。在实际应用中，可以根据具体情况选择合适的剪枝方法和参数。


### 2.随机森林
随机森林（Random Forest）是一种集成学习算法，主要用于分类和回归任务。它通过构建多个决策树，并结合这些树的预测结果来提高模型的准确性和鲁棒性。随机森林由Leo Breiman和Adele Cutler在2001年提出，是一种基于决策树的集成方法。

#### 随机森林的基本概念

1. **决策树（Decision Tree）**：
   - 决策树是一个树形结构，其中每个节点表示一个特征的测试，每个分支表示一个测试结果，每个叶节点表示一个类标签（用于分类）或一个数值（用于回归）。
   - 决策树的构建过程包括选择最佳特征进行分裂，直到满足停止条件（如达到最大深度或叶节点样本数小于某个阈值）。

2. **集成学习（Ensemble Learning）**：
   - 集成学习通过结合多个模型的预测结果来提高整体性能。随机森林属于集成学习的一种方法，具体来说是基于Bagging（Bootstrap Aggregating）的集成方法。

#### 随机森林的工作原理

1. **Bagging（Bootstrap Aggregating）**：
   - 随机森林通过Bagging方法生成多个决策树。Bagging的基本思想是通过自助采样（Bootstrap Sampling）从原始训练集生成多个子集，每个子集用于训练一个决策树。
   - 自助采样是指从原始训练集中有放回地随机抽取样本，生成的每个子集与原始训练集大小相同，但可能包含重复样本。

2. **随机特征选择**：
   - 在构建每个决策树时，随机森林在每个节点分裂时随机选择一个特征子集，而不是使用所有特征。这样可以增加树之间的差异性，进一步提高模型的泛化能力。

3. **预测过程**：
   - 对于分类任务，随机森林通过对所有决策树的预测结果进行投票，选择出现次数最多的类作为最终预测结果。
   - 对于回归任务，随机森林通过对所有决策树的预测结果取平均值，作为最终预测结果。

#### 随机森林的优势

1. **高准确性**：
   - 随机森林通过集成多个决策树的预测结果，通常比单个决策树具有更高的准确性和鲁棒性。

2. **抗过拟合**：
   - 由于随机森林通过Bagging和随机特征选择增加了模型的多样性，减少了过拟合的风险。

3. **处理高维数据**：
   - 随机森林可以处理高维数据（高维数据是指具有大量特征（变量）的数据集。），并且在特征选择过程中具有内置的特征重要性评估机制。

4. **易于并行化**：
   - 随机森林的每个决策树可以独立地构建和预测，因此易于并行化处理，提高计算效率。

#### 随机森林的应用

随机森林广泛应用于各种分类和回归任务，如：

1. **分类任务**：
   - 图像分类
   - 文本分类
   - 医学诊断

2. **回归任务**：
   - 房价预测
   - 股票价格预测
   - 环境数据分析

#### 随机森林的参数调优

1. **树的数量（n_estimators）**：
   - 决定随机森林中决策树的数量。通常树的数量越多，模型的性能越好，但计算开销也会增加。

2. **最大深度（max_depth）**：
   - 限制每个决策树的最大深度。较大的深度可能导致过拟合，而较小的深度可能导致欠拟合。

3. **最小样本分裂数（min_samples_split）**：
   - 控制一个节点需要包含的最小样本数，以便进行进一步分裂。较大的值可以防止过拟合。

4. **最小叶节点样本数（min_samples_leaf）**：
   - 控制每个叶节点需要包含的最小样本数。较大的值可以防止过拟合。

5. **最大特征数（max_features）**：
   - 控制每次分裂时考虑的特征数。较小的特征数可以增加树之间的差异性，提高泛化能力。

#### 总结

随机森林是一种强大的集成学习算法，通过结合多个决策树的预测结果来提高模型的准确性和鲁棒性。它具有高准确性、抗过拟合、处理高维数据和易于并行化等优势，广泛应用于各种分类和回归任务。通过合理的参数调优，可以进一步提升随机森林的性能。

### 3. 梯度提升决策树

梯度提升决策树（Gradient Boosting Decision Trees, GBDT）是一种强大的机器学习算法，广泛应用于分类和回归任务。GBDT通过逐步构建一系列弱学习器（通常是决策树），并结合它们的预测结果来提高模型的整体性能。它是提升方法（Boosting）的一个具体实现，提升方法的核心思想是通过加权组合多个弱学习器来构建一个强学习器。

#### GBDT的基本概念

1. **弱学习器（Weak Learner）**：
   - 弱学习器是指在单独使用时性能不佳的简单模型。GBDT通常使用决策树作为弱学习器，每棵树的深度较浅（即为弱学习器）。

2. **提升方法（Boosting）**：
   - 提升方法通过逐步构建和加权组合多个弱学习器来提高整体模型的性能。每个新加入的弱学习器都是在前一轮的基础上进行改进的。

3. **梯度提升（Gradient Boosting）**：
   - 梯度提升通过优化损失函数来逐步构建模型。在每一步中，GBDT通过拟合当前模型的负梯度（即损失函数的梯度）来构建新的弱学习器，从而减少模型的误差。

#### GBDT的工作原理

1. **初始化模型**：
   - 初始化一个简单的模型，通常是使用训练数据的均值作为初始预测值。

2. **迭代构建弱学习器**：
   - 在每一轮迭代中，计算当前模型的残差（即预测值与实际值之间的差异）。
   - 使用残差作为新的目标变量，训练一个新的决策树来拟合这些残差。
   - 将新决策树的预测结果加权后加入到当前模型中，更新模型的预测值。

3. **更新模型**：
   - 在每一轮迭代中，模型都会更新预测值，使之更接近实际值。
   - 通过逐步减小残差，模型的性能不断提高。

4. **最终模型**：
   - 当达到预定的迭代次数或模型的性能不再显著提升时，停止迭代，最终模型由所有弱学习器的加权组合构成。

#### GBDT的优势

1. **高准确性**：
   - 由于GBDT逐步优化模型的损失函数，它通常具有很高的预测准确性，特别是在处理复杂的非线性关系时。

2. **抗过拟合**：
   - GBDT通过逐步构建和组合多个弱学习器，具有较强的抗过拟合能力。可以通过调节参数（如树的数量、深度和学习率）来控制模型的复杂度。

3. **灵活性**：
   - GBDT可以处理各种类型的数据，包括数值型、分类型和缺失值数据。它也可以使用不同的损失函数来适应不同的任务需求。

4. **特征重要性评估**：
   - GBDT可以提供特征重要性评估，帮助理解哪些特征对模型的预测结果贡献最大。

#### GBDT的应用

GBDT广泛应用于各种分类和回归任务，如：

1. **分类任务**：
   - 电子邮件垃圾分类
   - 图像识别
   - 客户流失预测

2. **回归任务**：
   - 房价预测
   - 销售量预测
   - 风险评估

#### GBDT的参数调优

1. **树的数量（n_estimators）**：
   - 决定模型中树的数量。通常树的数量越多，模型的性能越好，但计算开销也会增加。

2. **学习率（learning_rate）**：
   - 控制每棵树对最终模型的贡献。较小的学习率可以提高模型的泛化能力，但需要更多的树来达到同样的效果。

3. **最大深度（max_depth）**：
   - 限制每棵树的最大深度。较大的深度可能导致过拟合，而较小的深度可能导致欠拟合。

4. **最小样本分裂数（min_samples_split）**：
   - 控制一个节点需要包含的最小样本数，以便进行进一步分裂。较大的值可以防止过拟合。

5. **最小叶节点样本数（min_samples_leaf）**：
   - 控制每个叶节点需要包含的最小样本数。较大的值可以防止过拟合。

6. **子样本比例（subsample）**：
   - 控制每棵树使用的子样本比例。较小的子样本比例可以增加模型的多样性，提高泛化能力。

#### 总结

梯度提升决策树（GBDT）是一种强大的机器学习算法，通过逐步构建和加权组合多个弱学习器来提高模型的整体性能。GBDT具有高准确性、抗过拟合、灵活性和特征重要性评估等优势，广泛应用于各种分类和回归任务。通过合理的参数调优，可以进一步提升GBDT的性能。

每轮更新迭代不会改变原始模型，原始模型的稳定性和结果一直不会又变化，只是说我不短叠加学习器，达到稳定的效果。

所以最终模型下的回归流程，可以理解为，输入A顺序的经过原始模型/迭代模型1/.../迭代模型n后的输出结果
